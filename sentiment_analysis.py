# -*- coding: utf-8 -*-
"""sentiment_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UREPcEciivlzed7OUdUJyLHELV1kRowZ
"""

# Online Shopping Reviews
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from wordcloud import WordCloud
from tqdm import tqdm

# NLP
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from textblob import TextBlob

# ML
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Download NLTK data
nltk_packages = ["punkt", "stopwords", "punkt_tab"]
for pkg in nltk_packages:
    try:
        nltk.data.find(f'tokenizers/{pkg}')
    except:
        nltk.download(pkg)

STOPWORDS = set(stopwords.words('english'))
STEMMER = PorterStemmer()

# Load and clean dataset
DATA_PATH = "/content/product_reviews_1000.csv"
df = pd.read_csv(DATA_PATH)

print("Initial shape:", df.shape)
df.drop_duplicates(inplace=True)
df = df.dropna(subset=['Review Text'])
df.reset_index(drop=True, inplace=True)
print("Cleaned shape: ", df.shape)
print(df.head())

# Preprocess

def normalize_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

def tokenize(text):
    return word_tokenize(text)

def remove_stopwords(tokens):
    return [t for t in tokens if t not in STOPWORDS and t.isalpha()]

def stem_tokens(tokens):
    return [STEMMER.stem(t) for t in tokens]

# Feature extraction
word_counts = []
avg_word_lengths = []
special_char_counts = []
sentiments = []
processed_texts = []

for text in tqdm(df['Review Text']):
    norm = normalize_text(text)
    tokens = tokenize(norm)
    tokens = remove_stopwords(tokens)
    stems = stem_tokens(tokens)
    processed_texts.append(' '.join(stems))


    words_no_punct = [w for w in tokens if w.isalpha()]
    word_counts.append(len(words_no_punct))
    avg_word_lengths.append(np.mean([len(w) for w in words_no_punct]) if words_no_punct else 0)
    special_char_counts.append(sum(1 for c in text if not c.isalnum() and not c.isspace()))

    tb = TextBlob(text)
    polarity = tb.sentiment.polarity
    if polarity > 0.05:
        sentiments.append('Positive')
    elif polarity < -0.05:
        sentiments.append('Negative')
    else:
        sentiments.append('Neutral')

df['Processed_Text'] = processed_texts
df['Word_Count'] = word_counts
df['Avg_Word_Length'] = avg_word_lengths
df['Special_Char_Count'] = special_char_counts
df['Predicted_Sentiment'] = sentiments

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df['Processed_Text'])

# Visualization
# WordCloud
all_text = ' '.join(df['Processed_Text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)
plt.figure(figsize=(15,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Common Words in Reviews')
plt.show()

# Sentiment count plot
sns.countplot(data=df, x='Predicted_Sentiment')
plt.title('Sentiment Distribution (TextBlob)')
plt.show()

# Word count distribution by sentiment
sns.boxplot(x='Predicted_Sentiment', y='Word_Count', data=df)
plt.title('Word Count Distribution by Sentiment')
plt.show()

# Sentiment Classification

X_features = pd.DataFrame({
    'Word_Count': df['Word_Count'],
    'Avg_Word_Length': df['Avg_Word_Length'],
    'Special_Char_Count': df['Special_Char_Count']
})
from scipy.sparse import hstack
X = hstack([X_features, X_tfidf])

y = df['Label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = RandomForestClassifier(n_estimators=200, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))